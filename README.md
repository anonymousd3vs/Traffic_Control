# Intelligent Traffic Management System

Production-ready **ONNX-optimized** vehicle + ambulance detection & tracking system with **dual-mode operation**:

- **Zone-Based Mode**: Lane-specific filtering with intelligent zone counting
- **Line-Based Mode**: Full-frame detection with line-crossing counting

Built for Indian traffic conditions with optimized ONNX models for faster inference.

## üöÄ Core Features

| Capability                          | Status                                     |
| ----------------------------------- | ------------------------------------------ |
| ONNX-optimized inference            | ‚úÖ (3-5x faster than PyTorch)              |
| **Video-Specific Configurations**   | ‚úÖ **NEW!** Each video gets own config     |
| **Zone-Based Lane Filtering**       | ‚úÖ **NEW!** Interactive lane configuration |
| **Dual-Mode Operation**             | ‚úÖ **NEW!** Zone-based + Line-based        |
| Vehicle detection (YOLOv11n ONNX)   | ‚úÖ                                         |
| ByteTrack tracking                  | ‚úÖ                                         |
| Ambulance detection (YOLOv11n ONNX) | ‚úÖ                                         |
| Interactive lane configuration tool | ‚úÖ **NEW!** Visual polygon drawing         |
| Zone-based vehicle counting         | ‚úÖ **NEW!** Movement-based counting        |
| Line-crossing counting (legacy)     | ‚úÖ                                         |
| Real-time FPS monitoring            | ‚úÖ                                         |
| Auto-prompt for missing configs     | ‚úÖ **NEW!** User-friendly workflow         |

## üéØ Quick Start

### 1) Clone & Setup

```bash
git clone https://github.com/anonymousd3vs/Traffic_Control.git
cd Traffic_Control
```

### 2) Install Dependencies

```bash
# Windows PowerShell
python -m venv .venv
.venv\Scripts\Activate

# Install requirements
pip install -r requirements.txt
```

### 3) Download/Prepare ONNX Models

Required models in `optimized_models/` folder:

- `yolo11n_optimized.onnx` - Vehicle detection (fast)
- `indian_ambulance_yolov11n_best_optimized.onnx` - Ambulance detection

_(Use `python -m core.optimization.model_optimizer` to convert PyTorch models to ONNX if needed)_

---

## üö¶ Two Operating Modes

### **Mode 1: Zone-Based (Lane Filtering)** üéØ

**When to use:** Monitor specific lanes, optimize performance by filtering irrelevant areas

**Setup & Run:**

```bash
# Option 1: Let the system prompt you (EASIEST!)
python run_detection.py --source "videos/your_video.mp4"
# If no config exists, you'll be prompted to:
#   1. Configure lane now
#   2. Continue without filtering
#   3. Quit

# Option 2: Quick automated setup
python scripts/quick_lane_setup.py "videos/your_video.mp4"
# This will:
#   1. Open interactive lane configuration tool
#   2. Let you draw lane polygon with mouse clicks
#   3. Automatically run detection with the configured lane

# Option 3: Manual step-by-step
# Step 1: Configure lane boundaries (one-time per video)
python -m shared.config.lane_config_tool --video "videos/your_video.mp4"

# Step 2: Run detection with lane filtering
python run_detection.py --source "videos/your_video.mp4" --output "output.mp4"
```

**üéØ Video-Specific Configurations:**

Each video gets its own configuration file! Once configured, the system automatically uses the correct config for each video.

```bash
# Configure different videos
python -m shared.config.lane_config_tool --video "videos/video1.mp4"
python -m shared.config.lane_config_tool --video "videos/video2.mp4"

# Run them anytime - each uses its own config automatically!
python run_detection.py --source "videos/video1.mp4"
python run_detection.py --source "videos/video2.mp4"
```

See [docs/VIDEO_SPECIFIC_CONFIGS.md](docs/VIDEO_SPECIFIC_CONFIGS.md) for details.

**Features:**

- ‚úÖ Detects vehicles **only inside green lane polygon**
- ‚úÖ Zone-based counting (counts after ‚â•50 pixels movement)
- ‚úÖ No counting line visible (clean interface)
- ‚úÖ Status: "Mode: Zone-Based"
- ‚úÖ Better performance (processes only relevant detections)

---

### **Mode 2: Line-Based (Full Frame)** üìä

**When to use:** Count all vehicles across entire road, general traffic monitoring

**Run:**

```bash
# Use --no-filter flag to force normal mode
python run_detection.py --source "videos/your_video.mp4" --no-filter --output "output.mp4"

# Or run without any lane configuration
python run_detection.py --source "videos/your_video.mp4" --output "output.mp4"
```

**Features:**

- ‚úÖ Detects vehicles **in entire frame**
- ‚úÖ Line-crossing counting (counts when crossing yellow line)
- ‚úÖ Yellow horizontal counting line visible
- ‚úÖ Status: "Mode: Line-Based"
- ‚úÖ Classic traffic counting approach

---

## üéÆ Runtime Controls

**During Detection:**

- **Q**: Quit and save
- **R**: Reset vehicle count
- **S**: Save screenshot
- **ESC**: Exit

**During Lane Configuration:**

- **Left Click**: Add lane boundary point
- **Right Click**: Remove last point
- **ESC**: Save configuration and exit
- **Q**: Quit without saving

## ÔøΩ Performance

### ONNX Optimization Benefits

- **3-5x faster** than PyTorch models
- **Lower memory usage**
- **Better CPU performance**
- Real-time processing on standard hardware

### Typical Performance

| Hardware        | Zone-Based Mode | Line-Based Mode | Notes                 |
| --------------- | --------------- | --------------- | --------------------- |
| CPU (Intel i5+) | 15-20 FPS       | 12-18 FPS       | ONNX optimized        |
| GPU (NVIDIA)    | 30-45 FPS       | 25-35 FPS       | Full frame processing |
| Edge Devices    | 8-12 FPS        | 6-10 FPS        | Raspberry Pi 4+       |

**Zone-Based Mode is faster** because it processes fewer detections!

---

## üõ† Command Line Reference

### Main Detection Script: `final_tracking_onnx.py`

```bash
python final_tracking_onnx.py [OPTIONS]
```

**Options:**

| Flag            | Type   | Default                   | Description                                                    |
| --------------- | ------ | ------------------------- | -------------------------------------------------------------- |
| `--source`      | string | `"0"`                     | Video source: camera index (`0`), video file path, or RTSP URL |
| `--output`      | string | `""`                      | Output video file path (optional)                              |
| `--lane-config` | string | `config/lane_config.json` | Path to lane configuration file                                |
| `--no-filter`   | flag   | `False`                   | **Force line-based mode**, ignore lane config                  |

### Lane Configuration Tool: `lane_config_tool.py`

```bash
python lane_config_tool.py --video <path> [OPTIONS]
```

**Options:**

| Flag       | Type   | Default                   | Description                          |
| ---------- | ------ | ------------------------- | ------------------------------------ |
| `--video`  | string | _required_                | Path to video file for configuration |
| `--config` | string | `config/lane_config.json` | Output configuration file path       |

### Quick Setup Script: `quick_lane_setup.py`

```bash
python quick_lane_setup.py <video_path>
```

Automated workflow: configure lane ‚Üí run detection in one command!

---

## ÔøΩ Usage Examples

### Example 1: Quick Lane Setup (Recommended)

```bash
python quick_lane_setup.py "videos/highway.mp4"
```

Interactive workflow that guides you through configuration and detection.

### Example 2: Configure Lane for Future Use

```bash
# Configure once
python lane_config_tool.py --video "videos/traffic.mp4"

# Run multiple times with same config
python final_tracking_onnx.py --source "videos/traffic.mp4" --output "result1.mp4"
python final_tracking_onnx.py --source "videos/traffic.mp4" --output "result2.mp4"
```

### Example 3: Compare Both Modes

```bash
# Zone-based mode (lane filtering)
python final_tracking_onnx.py --source "videos/test.mp4" --output "zone_mode.mp4"

# Line-based mode (full frame)
python final_tracking_onnx.py --source "videos/test.mp4" --output "line_mode.mp4" --no-filter
```

### Example 4: Process Camera Feed

```bash
# Webcam with lane filtering
python final_tracking_onnx.py --source 0

# Webcam without filtering
python final_tracking_onnx.py --source 0 --no-filter
```

### Example 5: RTSP Stream

```bash
python final_tracking_onnx.py --source "rtsp://camera-ip:554/stream" --output "stream_output.mp4"
```

---

## üé® Visual Features

### Vehicle Detection Classes

- **Cars** - Green bounding boxes
- **Motorcycles** - Orange bounding boxes
- **Bicycles** - Cyan bounding boxes
- **Buses** - Magenta bounding boxes
- **Trucks** - Blue bounding boxes

### Ambulance Detection

- **Ambulance** - Red bounding box with "üö® AMBULANCE" label
- High-priority visual indicator

### Visual Overlays

**Zone-Based Mode:**

- Green semi-transparent lane polygon
- Colored vehicle trajectories (tracking history)
- Vehicle bounding boxes with IDs
- Stats panel: "Mode: Zone-Based", "Total in Zone: X"

**Line-Based Mode:**

- Yellow horizontal counting line
- Colored vehicle trajectories
- Vehicle bounding boxes with IDs
- Stats panel: "Mode: Line-Based", "Total Crossed: X"

---

## üèó System Architecture

### Detection Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     INPUT VIDEO FRAME                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ONNX Vehicle Detection (YOLOv11n)              ‚îÇ
‚îÇ         ‚Ä¢ Optimized inference (3-5x faster)                 ‚îÇ
‚îÇ         ‚Ä¢ Detects: car, bus, truck, motorcycle, bicycle     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         LANE FILTERING (if zone-based mode enabled)         ‚îÇ
‚îÇ         ‚Ä¢ Point-in-polygon test for each detection          ‚îÇ
‚îÇ         ‚Ä¢ Filters out vehicles outside configured lane      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BYTETRACK TRACKING                        ‚îÇ
‚îÇ         ‚Ä¢ Assigns unique IDs to vehicles                    ‚îÇ
‚îÇ         ‚Ä¢ Maintains trajectory history (20 frames)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   VEHICLE COUNTING                           ‚îÇ
‚îÇ   Zone Mode: Movement-based (‚â•50 pixels through zone)       ‚îÇ
‚îÇ   Line Mode: Line-crossing detection                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            ONNX Ambulance Detection (YOLOv11n)              ‚îÇ
‚îÇ         ‚Ä¢ High-priority emergency vehicle detection         ‚îÇ
‚îÇ         ‚Ä¢ Visual/audio alert capability                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 VISUALIZATION & OUTPUT                       ‚îÇ
‚îÇ         ‚Ä¢ Draw bounding boxes, trajectories, stats          ‚îÇ
‚îÇ         ‚Ä¢ Overlay lane polygon or counting line             ‚îÇ
‚îÇ         ‚Ä¢ Save to video file (if --output specified)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Project Structure

```
traffic_control/
‚îú‚îÄ‚îÄ core/                           # Core detection & tracking
‚îÇ   ‚îú‚îÄ‚îÄ detectors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ onnx_detector.py       # ONNX model inference
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ traffic_detector.py    # Main detection system
‚îÇ   ‚îú‚îÄ‚îÄ trackers/                  # Vehicle tracking (future)
‚îÇ   ‚îî‚îÄ‚îÄ optimization/
‚îÇ       ‚îî‚îÄ‚îÄ model_optimizer.py     # PyTorch ‚Üí ONNX converter
‚îÇ
‚îú‚îÄ‚îÄ dashboard/                      # Web dashboard (Phase 1)
‚îÇ   ‚îú‚îÄ‚îÄ backend/                   # Flask + SocketIO server
‚îÇ   ‚îî‚îÄ‚îÄ frontend/                  # React app
‚îÇ
‚îú‚îÄ‚îÄ traffic_signals/                # Signal control (Phase 2)
‚îÇ   ‚îú‚îÄ‚îÄ core/                      # State machine & priority
‚îÇ   ‚îî‚îÄ‚îÄ hardware/                  # GPIO, Modbus, simulator
‚îÇ
‚îú‚îÄ‚îÄ shared/                         # Shared utilities
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lane_config_tool.py    # Interactive lane config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_config_manager.py # Video-specific configs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ check_lane_config.py   # Config diagnostics
‚îÇ   ‚îî‚îÄ‚îÄ utils/                     # Common utilities
‚îÇ
‚îú‚îÄ‚îÄ tests/                          # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/                      # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ integration/               # Integration tests
‚îÇ   ‚îî‚îÄ‚îÄ performance/               # Performance tests
‚îÇ
‚îú‚îÄ‚îÄ scripts/                        # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ migrate_config.py          # Config migration
‚îÇ   ‚îú‚îÄ‚îÄ quick_lane_setup.py        # Quick lane setup
‚îÇ   ‚îî‚îÄ‚îÄ setup_environment.py       # Environment setup
‚îÇ
‚îú‚îÄ‚îÄ config/                         # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ lane_config_*.json         # Video-specific lane configs
‚îÇ   ‚îú‚îÄ‚îÄ lane_configs_master.json   # Master config registry
‚îÇ   ‚îú‚îÄ‚îÄ development.yaml           # Dev environment config
‚îÇ   ‚îú‚îÄ‚îÄ production.yaml            # Prod environment config
‚îÇ   ‚îî‚îÄ‚îÄ debug_config.json          # Debug settings
‚îÇ
‚îú‚îÄ‚îÄ models/                         # PyTorch models
‚îú‚îÄ‚îÄ optimized_models/               # ONNX models
‚îú‚îÄ‚îÄ videos/                         # Test videos
‚îú‚îÄ‚îÄ output/                         # Output videos
‚îú‚îÄ‚îÄ docs/                           # Documentation
‚îÇ
‚îú‚îÄ‚îÄ run_detection.py                # Main entry: run detection
‚îú‚îÄ‚îÄ run_dashboard.py                # Main entry: start dashboard
‚îú‚îÄ‚îÄ run_signals.py                  # Main entry: signal simulator
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ ROADMAP.md                      # Development roadmap
‚îî‚îÄ‚îÄ README.md                       # This file
```

**New Modular Structure** (Updated Oct 9, 2025):

- ‚úÖ Clean separation of concerns
- ‚úÖ Ready for dashboard & signal development
- ‚úÖ Organized test structure
- ‚úÖ Scalable and maintainable

See [REORGANIZATION_COMPLETE.md](REORGANIZATION_COMPLETE.md) for details.

## üéõ Examples

Higher recall (lower vehicle confidence) in accuracy mode:

```
python final_tracking_detection.py --conf 0.18 --accuracy --source videos/rushing.mp4
```

Fast mode + tighter ambulance interval (every frame):

```
python final_tracking_detection.py --fast --ambulance-interval 1 --source videos/rushing.mp4
```

Override ambulance model:

```
python final_tracking_detection.py --ambulance-model models/custom_amb.pt
```

### Performance Optimization

- CPU: prefer `--super-fast` (YOLO11n)
- GPU: larger `--imgsz` + `--accuracy` for recall
- Memory: tune processing size and ROI

## ÔøΩ Ambulance Detection Logic

Default: raw custom YOLOv11m weights at adaptive confidence (starts ~0.07, decays slightly if starved, gently rises after hits, capped to avoid missing).

Fallback: YOLOv8 custom model may produce higher confidences; used when primary misses and fallback is enabled.

Ensemble: `--ambulance-ensemble` invokes the enhanced detector (slower; includes visual feature heuristics) for evaluation scenarios.

## üìà Runtime Stats Overlay

Displayed:

- FPS / frame latency
- Active / confirmed vehicles
- Recent ambulance event flag
- (Optional) snapshot saving to `logs/` when enabled

## üö¶ Integration Potential

Foundation for adaptive signals, emergency corridor automation, and density analytics. JSON/event streaming layer can be added externally (not bundled to keep core lean).

## üì¶ Deployment Notes

- Keep model weights outside Git history (use release assets or manual placement)
- GPU recommended for accuracy mode at 832 resolution
- For edge devices drop to `--fast` and `yolo11n.pt`

---

## ÔøΩ Lane Configuration Guide

### Interactive Lane Setup

The lane configuration tool provides a visual interface to define lane boundaries:

**Steps:**

1. Launch tool: `python lane_config_tool.py --video "your_video.mp4"`
2. **Left-click** on frame to add boundary points around your lane
3. **Right-click** to remove the last point if needed
4. Press **ESC** to save configuration
5. Creates `config/lane_config.json` + preview image

### Best Practices

‚úÖ **DO:**

- Cover the **entire lane height** you want to monitor
- Draw polygon with 4-8 points for best coverage
- Use `check_lane_config.py` to verify coverage

‚ùå **DON'T:**

- Make polygon too small (causes missed detections)
- Use less than 3 points (minimum required)

### Diagnostic Tool

```bash
python check_lane_config.py  # Verify your lane configuration
```

---

## üìà On-Screen Statistics

**Zone-Based Mode:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DETECTION STATS        ‚îÇ
‚îÇ FPS: 18.5             ‚îÇ
‚îÇ Active Objects: 5      ‚îÇ
‚îÇ Total in Zone: 23      ‚îÇ
‚îÇ Mode: Zone-Based       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Line-Based Mode:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DETECTION STATS        ‚îÇ
‚îÇ FPS: 15.2             ‚îÇ
‚îÇ Active Objects: 12     ‚îÇ
‚îÇ Total Crossed: 45      ‚îÇ
‚îÇ Mode: Line-Based       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Requirements

### System Requirements

- **Python:** 3.8+ (3.10+ recommended)
- **OS:** Windows, Linux, macOS
- **RAM:** 4GB minimum, 8GB recommended
- **Storage:** 2GB for models and dependencies

### Python Dependencies

```
opencv-python>=4.8.0
numpy>=1.24.0
onnxruntime>=1.16.0
filterpy>=1.4.5
scipy>=1.10.0
```

Install:

```bash
pip install -r requirements.txt
```

### ONNX Model Conversion (Optional)

```bash
pip install -r requirements_optimize.txt
python optimize_models.py
```

---

## üö¶ Phase 2: Traffic Signal Control System ‚úÖ

**Status:** COMPLETE & READY FOR DEPLOYMENT

### Quick Start - Test the Simulator

```bash
# Install pygame (if not already installed)
pip install pygame

# Run the simulator
python -m traffic_signals.hardware.signal_simulator
```

**Keyboard Controls:**

- `N/S/E/W` - Select direction (North, South, East, West)
- `A` - Trigger ambulance from selected direction
- `SPACE` - Pause/Resume
- `R` - Reset all signals
- `H` - Help
- `Q` - Quit

### Features Implemented

‚úÖ **Signal State Machine** - 4-state FSM (RED, YELLOW, GREEN, EMERGENCY)
‚úÖ **Priority Manager** - Multi-signal coordination with emergency override
‚úÖ **Visual Simulator** - Pygame-based 4-way intersection visualization
‚úÖ **GPIO Controller** - Raspberry Pi GPIO interface for real hardware
‚úÖ **Modbus Controller** - Industrial Modbus TCP/RTU support
‚úÖ **REST API Handler** - 10+ endpoints for signal control and monitoring
‚úÖ **Emergency Tracking** - Automatic ambulance priority with 45-second duration
‚úÖ **Statistics & Metrics** - Real-time and historical data collection

### Integration with Detection System

```python
from traffic_signals import PriorityManager

# In your detection loop
signals = PriorityManager()
signals.register_signal('north', 'north')
signals.register_signal('south', 'south')
signals.register_signal('east', 'east')
signals.register_signal('west', 'west')
signals.start_all_signals()

# Update signals every frame
while processing_video:
    signals.update()

    # When ambulance detected
    if ambulance_detected and confidence > 0.8:
        signals.activate_emergency(
            ambulance_id=f"amb_{timestamp}",
            direction="north",  # Determine from detection
            confidence=confidence
        )
```

### Normal Signal Cycle

```
RED:    34 seconds  üî¥
GREEN:  30 seconds  üü¢
YELLOW:  4 seconds  üü°
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: 68 seconds per direction pair
```

### Emergency Mode

```
[Any State] ‚Üí EMERGENCY (GREEN) ‚Üí 45 seconds üö®
```

### Files in traffic_signals/

```
traffic_signals/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ signal_state_machine.py      # Individual signal FSM (370+ lines)
‚îÇ   ‚îî‚îÄ‚îÄ priority_manager.py          # Multi-signal coordination (380+ lines)
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ signal_api.py                # REST API handler (450+ lines)
‚îî‚îÄ‚îÄ hardware/
    ‚îú‚îÄ‚îÄ signal_simulator.py          # Visual simulator - Pygame (600+ lines)
    ‚îú‚îÄ‚îÄ realistic_junction_simulator.py  # Advanced simulator
    ‚îú‚îÄ‚îÄ gpio_controller.py           # Raspberry Pi GPIO (140+ lines)
    ‚îî‚îÄ‚îÄ modbus_controller.py         # Modbus TCP/RTU (180+ lines)
```

### API Endpoints

| Method | Endpoint                   | Description                 |
| ------ | -------------------------- | --------------------------- |
| GET    | `/api/signals/status`      | Get all signal states       |
| GET    | `/api/signals/emergencies` | Get active emergencies      |
| POST   | `/api/signals/emergency`   | Activate ambulance priority |
| GET    | `/api/signals/statistics`  | Get system statistics       |
| POST   | `/api/signals/reset`       | Reset all signals           |

### Dashboard Integration

The dashboard now displays:

- Real-time signal status for all 4 directions
- Active emergency alerts with duration countdown
- Emergency history and statistics
- 3-way integration: Detection + Signals + Dashboard

### Hardware Support

- **Simulator Mode** (Testing) - Pygame visualization
- **Raspberry Pi (GPIO)** - Direct GPIO pin control
- **Industrial Modbus** - TCP/RTU protocol support

---

## üö¶ Real-World Applications

- üö¶ **Smart Traffic Signals** - Adaptive signal timing ‚úÖ IMPLEMENTED
- üöë **Emergency Response** - Automatic ambulance priority ‚úÖ IMPLEMENTED
- üìä **Traffic Analytics** - Lane-specific monitoring
- üé• **Surveillance Systems** - Camera infrastructure integration
- üèóÔ∏è **Smart Cities** - Urban planning data

---

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/NewFeature`)
3. Commit changes (`git commit -m 'Add NewFeature'`)
4. Push to branch (`git push origin feature/NewFeature`)
5. Open Pull Request

---

## üìÑ License

MIT License - See LICENSE file for details

---

## üìö Documentation

Comprehensive documentation available in `docs/` folder:

- `ZONE_BASED_COUNTING.md` - Zone-based mode detailed guide
- `LANE_BASED_OPTIMIZATION.md` - Technical implementation details
- `IMPLEMENTATION_SUMMARY.md` - System architecture overview

---

## üôè Acknowledgments

- Built for Indian traffic conditions üáÆüá≥
- ONNX-optimized for production deployment
- Dual-mode operation for maximum flexibility

---

---

## üê≥ Docker Deployment

**Status:** ‚úÖ Production Ready

Complete Docker setup for seamless deployment across Windows, Linux, macOS, and Raspberry Pi 5.

### Quick Start - Docker Compose

```bash
# Clone and navigate
git clone https://github.com/anonymousd3vs/Traffic_Control.git
cd Traffic_Control

# Start all services (Backend + Frontend + Detection)
docker-compose -f docker/docker-compose.yml up -d

# Open in browser
# Frontend: http://localhost:3000
# Backend API: http://localhost:8765
```

### Docker Images

| Service  | Image              | Port | Purpose                |
| -------- | ------------------ | ---- | ---------------------- |
| Backend  | `python:3.11-slim` | 8765 | Detection engine + API |
| Frontend | `node:18 ‚Üí nginx`  | 3000 | React dashboard        |
| Network  | Bridge             | -    | Internal communication |

### Deployment Options

**Option 1: Linux/Windows/macOS (Standard)**

```bash
docker-compose -f docker/docker-compose.yml up
```

**Option 2: Raspberry Pi 5 (ARM64 Optimized)**

```bash
docker-compose -f docker/raspberrypi/docker-compose.rpi.yml up
```

**Option 3: Build & Deploy Custom**

```bash
# Build images
docker build -f docker/backend/Dockerfile -t traffic-backend .
docker build -f docker/frontend/Dockerfile -t traffic-frontend .

# Run backend
docker run -d -p 8765:8765 \
  -v $(pwd)/videos:/app/videos \
  -v $(pwd)/output:/app/output \
  traffic-backend

# Run frontend
docker run -d -p 3000:3000 traffic-frontend
```

### Environment Configuration

Create `.env` file:

```env
DETECTION_MODE=zone  # or 'line'
GPU_ENABLED=false
MODEL_PATH=/app/optimized_models
VIDEO_INPUT=/app/videos
VIDEO_OUTPUT=/app/output
```

See `docker/README.md` and `docker/DOCKER_GUIDE.md` for complete documentation.

---

## üéõ Dashboard Overview

**Status:** ‚úÖ Production Ready

A fully functional real-time dashboard with:

- **Live Video Stream** - Processed video feed with detection overlays (5 FPS optimized)
- **System Metrics** - FPS, active vehicles, total count, detection mode
- **Emergency Alerts** - Visual and audio alerts for ambulance detection
- **Traffic Flow Charts** - Historical data visualization
- **WebSocket Communication** - Sub-second latency updates

### Quick Start

```bash
python run_dashboard.py
# Opens: http://localhost:5173 (Frontend)
# Backend: http://localhost:8765
```

See `dashboard/README.md` for complete documentation.

---

**Version:** 2.2 - Phase 2 Complete (Traffic Signals + Dashboard)  
**Last Updated:** October 26, 2025  
**Repository:** [github.com/anonymousd3vs/Traffic_Control](https://github.com/anonymousd3vs/Traffic_Control)
